{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7228f577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.session.SparkSession at 0x23f824ef550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "  .remote(\"sc://192.168.1.7:15002\") \\\n",
    "  .appName(\"UDFTransformation\") \\\n",
    "  .config(\"spark.sql.ansi.enabled\", \"false\") \\\n",
    "  .config(\"spark.sql.execution.pythonUDF.arrow.enabled\", \"true\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "# limit() shows a nice HTML table in Jupyter, while show() prints plain text  \n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c491899b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>value</th></tr>\n",
       "<tr><td>4</td></tr>\n",
       "<tr><td>5</td></tr>\n",
       "<tr><td>6</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|value|\n",
       "+-----+\n",
       "|    4|\n",
       "|    5|\n",
       "|    6|\n",
       "+-----+"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(returnType='int')\n",
    "def mult_by_3(s: int) -> int:\n",
    "  return s * 3\n",
    "\n",
    "df = spark.createDataFrame([(4, ), (5, ), (6, )], ['value'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d627dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>value</th><th>value_x3</th></tr>\n",
       "<tr><td>4</td><td>12</td></tr>\n",
       "<tr><td>5</td><td>15</td></tr>\n",
       "<tr><td>6</td><td>18</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+--------+\n",
       "|value|value_x3|\n",
       "+-----+--------+\n",
       "|    4|      12|\n",
       "|    5|      15|\n",
       "|    6|      18|\n",
       "+-----+--------+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = df.withColumn('value_x3', mult_by_3(df.value))\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2b571d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\plancha\\spark-lab1\\.venv\\lib\\site-packages\\pyspark\\pandas\\__init__.py:43: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>value</th><th>value_x3</th><th>value_minus_2</th></tr>\n",
       "<tr><td>4</td><td>12</td><td>2</td></tr>\n",
       "<tr><td>5</td><td>15</td><td>3</td></tr>\n",
       "<tr><td>6</td><td>18</td><td>4</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+--------+-------------+\n",
       "|value|value_x3|value_minus_2|\n",
       "+-----+--------+-------------+\n",
       "|    4|      12|            2|\n",
       "|    5|      15|            3|\n",
       "|    6|      18|            4|\n",
       "+-----+--------+-------------+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(\"int\")\n",
    "def sub_2(s: pd.Series) -> pd.Series:\n",
    "  return s - 2\n",
    "\n",
    "dffs = dff.withColumn('value_minus_2', sub_2(dff.value))\n",
    "dffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db40eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
